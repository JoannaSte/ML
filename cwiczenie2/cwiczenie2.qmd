---
title: "Projekt ML"
subtitle: "Cwiczenie 2"
date: today
author: "Joanna Stępień"

format: 
  html:
    
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Spis Treści"
    number-sections: true
    number-depth: 3
    code-fold: show
    code-summary: "Napis"
    code-tools: true
    code-block-bg: "grey50"
    code-block-border-left: "black"
    code-line-numbers: true
    code-copy: true
    html-math-method: katex
    embed-resources: true
    smooth-scroll: true
    anchor-sections: true
    citation: true
    theme: 
      light: cosmo
      dark: darkly
    fontsize: 1.0em
    linestretch: 1.5
execute: 
  
  eval: true
  warning: false
  echo: true
  error: false
  cache: true
editor_options: 
  chunk_output_type: console
---
---

## Ładujemy niezbędne pakiety
```{r}
library(tidymodels) 
library(skimr) 
library(GGally) 
library(openair) 
tidymodels_prefer()
```

## ładujemy zbiór danych,wybieramy konkretny rok, robimy analize wartości oraz usuwamy braki w danych

```{r}
air <- mydata |> selectByDate(year = 2002) 
air |> skim()
air
air <- air |> na.omit()
air
```

## Sprawdzamy korelacje pomiędzy nox i no2 i rysujemy dla nich wykres regresji liniowe. Wyszło nam że dane są mozno skorelowane

```{r}
set.seed(222)
air[sample(1:nrow(air), size = 300, replace = F),] |> 
  select(nox, no2) |> 
  ggpairs()

library(ggpubr)
# wykres regresji liniowej, do sprawdzenia danych 
set.seed(222)
air[sample(1:nrow(air), size = 300, replace = F),] |> 
  select(nox, no2) |> 
  ggplot(aes(nox, no2)) +
  geom_point() +
  geom_smooth(method = "lm", se = T, formula = y ~ x) + 
  stat_cor(label.x = 10, label.y = 80) + 
  stat_regline_equation(label.x = 10, label.y = 82) +
  theme_bw()
```

## Badamy rozkład wartości stężeń azotu 

```{r}
air |>    
  ggplot(aes(date, o3)) +     
  geom_line() +     
  theme_bw()
```

## Badamy zakres stężeń, labelujemy je w zależności czy są niskie i wysokie według przyjętej wartości 10 i liczymy ile jest stężeń niskich i wysokich

```{r}
air |> 
  pull(o3) |> 
  range()  

air <-
  air |>
  mutate(ozone = cut(
    o3,
    breaks = c(-0.1, 10, 53),
    labels = c("Niskie", "Wysokie")
  ))
air |> count(ozone)
```
## Dzielimy dataset na zbiór testowy i treningowy
```{r}
set.seed(222)
data_split <- initial_split(data = air, prop = 3/4)
train_data <- training(data_split)
test_data <-  testing(data_split)
```
## Tworzymy recepture, nie używamy daty bo jest problematyczną wartości i nox bo jest silnie skorelowane z inną zmienną w zbiorze
```{r}
air_recipe <- recipe(ozone ~ ., data = train_data) |> 
  update_role(date, nox, new_role="nowe_inf") 
```


## model, definiujemy przepływ pracy i trenujemy go zbiorem treningowym
```{r}
model <- 
  logistic_reg() |> 
  set_engine("glm")

model_work <- 
  workflow() |> 
  add_model(model) |> 
  add_recipe(air_recipe)

model_work_fit <-  
  model_work |> 
  fit(data = train_data)

model_work_fit |> 
  extract_fit_parsnip() |> 
  tidy()

model_work_fit|> 
  extract_recipe()
```

## Patrzymy na wyniki
```{r}
prediction <- 
  augment(model_work_fit, test_data)


prediction  |> 
  roc_curve(truth = ozone, .pred_Niskie) |> 
  autoplot()
```
